{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# mnist = input_data.read_data_sets('./MNIST_data/', one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "def xavier_init(n_features, n_components, const=1):\n",
    "    low = -const*np.sqrt(6.0/(n_features + n_components))\n",
    "    high = -low\n",
    "    return tf.random_uniform((n_features, n_components), minval=low, maxval=high)\n",
    "\n",
    "def gen_batches(data, data_corrupted, batch_size, data_label=None, random=True):\n",
    "    \"\"\" Divide input data into batches.\n",
    "\n",
    "    :param data: input scipy sparse matrix / numpy array / pd DataFrame\n",
    "    :param data_corrupted: input corrupted data, same type as data\n",
    "    :param batch_size: size of each batch, (0,1] or integer >=1\n",
    "    :param data_label: label of data, pd DataFrame / pd Series / numpy 1-d, or 2-d array\n",
    "\n",
    "    :return: data divided into batches\n",
    "\n",
    "    ..  note: data & data_corrupted must be of same type, but data_label can be any data type\n",
    "    \"\"\"\n",
    "    assert batch_size > 0.\n",
    "    assert data.shape[0] == data_corrupted.shape[0]\n",
    "    assert type(data) == type(data_corrupted),(type(data), type(data_corrupted))\n",
    "    if isinstance(data, pd.DataFrame): assert (data.index == data_corrupted.index).all()\n",
    "    if data_label is not None: assert data_label.ndim == 1 or data_label.shape[1] == 1\n",
    "\n",
    "    if batch_size < 1.: batch_size = max(round(data.shape[0] * batch_size), 1)\n",
    "    batch_size = int(batch_size)\n",
    "\n",
    "    index = list(range(0, data.shape[0]))\n",
    "    if random: np.random.shuffle(index)\n",
    "\n",
    "    for i in range(0, data.shape[0], batch_size):\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            batch_data = data.iloc[index[i:i + batch_size]]\n",
    "            batch_data_corrupted = data_corrupted.iloc[index[i:i + batch_size]]\n",
    "\n",
    "        else:\n",
    "            batch_data = data[index[i:i + batch_size]]\n",
    "            batch_data_corrupted = data_corrupted[index[i:i + batch_size]]\n",
    "\n",
    "        if data_label is not None:\n",
    "            if isinstance(data_label, pd.DataFrame) or isinstance(data_label, pd.Series):\n",
    "                batch_label = data_label.iloc[index[i:i + batch_size]]\n",
    "            else:\n",
    "                batch_label = data_label[index[i:i + batch_size]]\n",
    "            yield (batch_data, batch_data_corrupted, batch_label)\n",
    "\n",
    "        else:\n",
    "            yield (batch_data, batch_data_corrupted)\n",
    "            \n",
    "def salt_and_pepper_noise(X, v):\n",
    "    \"\"\" Apply salt and pepper noise to data in X, in other words a fraction v of elements of X\n",
    "    (chosen at random) is set to its maximum or minimum value according to a fair coin flip.\n",
    "    If minimum or maximum are not given, the min (max) value in X is taken.\n",
    "\n",
    "    :param X: array_like, Input data\n",
    "    :param v: int, fraction of elements to distort\n",
    "\n",
    "    :return: transformed data\n",
    "    \"\"\"\n",
    "    X_noise = X.tolil(True) if not isinstance(X, np.ndarray) else X.copy()\n",
    "    n_features = X.shape[1]\n",
    "\n",
    "    mn = X.min()\n",
    "    mx = X.max()\n",
    "\n",
    "    for i, sample in enumerate(X):\n",
    "        mask = np.random.randint(0, n_features, v)\n",
    "\n",
    "        for m in mask:\n",
    "\n",
    "            if np.random.random() < 0.5:\n",
    "                X_noise[i,m] = mn\n",
    "            else:\n",
    "                X_noise[i,m] = mx\n",
    "\n",
    "    return X_noise.tocsr() if not isinstance(X, np.ndarray) else X_noise\n",
    "\n",
    "def get_sparse_ind_val_shape(sparse_m):\n",
    "    \"\"\" get indices, values, shape of a sparse matrix for feeding tf sparse placeholder\n",
    "\n",
    "    :param sparse_m: input sparse matrix\n",
    "\n",
    "    :type any scipy sparse matrix, csr/csc/coo/lil\n",
    "\n",
    "    :return: tuple of indices, values, shape\n",
    "    \"\"\"\n",
    "    if not isinstance(sparse_m, sparse.csr_matrix):\n",
    "        sparse_m = sparse.csr_matrix(sparse_m)\n",
    "    sparse_m.sort_indices()\n",
    "\n",
    "    sparse_m = sparse.coo_matrix(sparse_m)\n",
    "    indices = np.column_stack((sparse_m.row, sparse_m.col))\n",
    "    values = sparse_m.data\n",
    "    shape = sparse_m.shape\n",
    "\n",
    "    return (indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingAutoencoder(object):\n",
    "    def __init__(self):\n",
    "        self.compress_factor = 100\n",
    "        self.xavier_init = 1\n",
    "        self.alpha = 1\n",
    "        self.learning_rate = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.loss_func = 'cross_entropy'\n",
    "        self.num_epochs = 10\n",
    "        self.corr_frac = 0.0\n",
    "        self.batch_size = 10\n",
    "        self.model_dir = 'dae'\n",
    "        self.model_name = './mnist/'\n",
    "        self.tf_saver = None\n",
    "\n",
    "    def fit(self, train_set, validation_set=None, train_set_label=None, validation_set_label=None):\n",
    "#         self.sparse_input = False if isinstance(train_set, np.ndarray) else True\n",
    "        self.sparse_input = True\n",
    "    \n",
    "        n_features = train_set.shape[1]\n",
    "        self.n_components = np.floor(n_features/self.compress_factor).astype(int)\n",
    "        self._build_model(n_features)\n",
    "        \n",
    "        with tf.Session() as self.tf_session:\n",
    "            self._initialize_tf_utilities_and_ops()\n",
    "            self._train_model(train_set, validation_set, train_set_label=train_set_label, validation_set_label=validation_set_label)\n",
    "            self.tf_saver.save(self.tf_session, self.model_name)\n",
    "\n",
    "    def _initialize_tf_utilities_and_ops(self):\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        self.tf_saver = tf.train.Saver()\n",
    "        self.tf_session.run(init_op)\n",
    "        \n",
    "    def _train_model(self, train_set, validation_set, train_set_label, validation_set_label):\n",
    "        corruption_ratio = np.round(self.corr_frac*train_set.shape[1]).astype(np.int)\n",
    "        \n",
    "        for i in range(self.num_epochs):\n",
    "            self.train_cost_batch = [], [], []\n",
    "            self.fraction_triplet_batch = []\n",
    "            self.num_triplet_batch = []\n",
    "            \n",
    "            self._run_train_step(train_set, train_set_label, corruption_ratio, i+1)\n",
    "            \n",
    "    def _run_train_step(self, train_set, train_set_label, corruption_ratio, epoch):\n",
    "        x_corrupted = self._corrupt_input(train_set, corruption_ratio)\n",
    "        batches = [_ for _ in gen_batches(train_set, x_corrupted, self.batch_size, data_label=train_set_label)]\n",
    "        \n",
    "        i = 1\n",
    "        for batch in batches:\n",
    "            if train_set_label is not None:\n",
    "                x_batch, x_corr_batch, x_batch_label = batch\n",
    "            else:\n",
    "                print('test')\n",
    "                x_batch, x_corr_batch = batch\n",
    "\n",
    "            if self.sparse_input:\n",
    "                train_feed = {self.input_data: get_sparse_ind_val_shape(x_batch),\n",
    "                              self.input_data_corr: get_sparse_ind_val_shape(x_corr_batch),\n",
    "                              self.input_label: x_batch_label}\n",
    "            else:\n",
    "                train_feed = {self.input_data: x_batch,\n",
    "                              self.input_data_corr: x_corr_batch,\n",
    "                              self.input_label: x_batch_label}\n",
    "\n",
    "            step, train_autoencoder_loss, train_triplet_loss, train_cost, fraction_triplet, num_triplet = self.tf_session.run(\n",
    "                [self.train_step, self.autoencoder_loss, self.triplet_loss, self.cost, self.fraction_triplet, self.num_triplet],\n",
    "                feed_dict=train_feed\n",
    "            )\n",
    "\n",
    "            self.train_cost_batch[0].append(train_cost)\n",
    "            self.train_cost_batch[1].append(train_autoencoder_loss)\n",
    "            self.train_cost_batch[2].append(train_triplet_loss)\n",
    "            self.fraction_triplet_batch.append(fraction_triplet)\n",
    "            self.num_triplet_batch.append(num_triplet)\n",
    "\n",
    "            i += 1\n",
    "            \n",
    "    def _corrupt_input(self, data, v):\n",
    "        x_corrupted = salt_and_pepper_noise(data, v)\n",
    "#         x_corrupted = None\n",
    "        return x_corrupted\n",
    "\n",
    "    def _build_model(self, n_features):\n",
    "        self.input_data, self.input_data_corr, self.input_label = self._create_placeholders()\n",
    "        self.W_, self.bh_, self.bv_ = self._create_variables(n_features)\n",
    "        \n",
    "        self._create_encode_layer()\n",
    "        self._create_decode_layer()\n",
    "        self._create_cost_function_node()\n",
    "        self._create_train_step_node()\n",
    "\n",
    "    def _create_placeholders(self):\n",
    "        _placeholder = tf.sparse.placeholder if self.sparse_input else tf.placeholder\n",
    "        input_data = _placeholder('float', name='x-input')\n",
    "        input_data_corr = _placeholder('float', name='x-input-corr')\n",
    "        input_label = tf.placeholder('float', name='x-input-label')\n",
    "        \n",
    "        return input_data, input_data_corr, input_label\n",
    "        \n",
    "    def _create_variables(self, n_features):\n",
    "        W_ = tf.Variable(xavier_init(n_features, self.n_components, self.xavier_init), name='enc-W')\n",
    "        bh_ = tf.Variable(tf.zeros([self.n_components]), name='hidden-bias')\n",
    "        bv_ = tf.Variable(tf.zeros([n_features]), name='visible-bias')\n",
    "        \n",
    "        return W_, bh_, bv_\n",
    "    \n",
    "    def _create_encode_layer(self):\n",
    "        _matmul = tf.sparse.matmul if self.sparse_input else tf.matmul\n",
    "        \n",
    "        with tf.name_scope('encode'):\n",
    "            _enc_act_func = tf.nn.tanh            \n",
    "            self.encode = _enc_act_func(_matmul(self.input_data_corr, self.W_) + self.bh_) - _enc_act_func(self.bh_)\n",
    "    \n",
    "    def _create_decode_layer(self):\n",
    "        with tf.name_scope('decode'):\n",
    "            _dec_act_func = tf.nn.tanh\n",
    "            self.decode = _dec_act_func(tf.matmul(self.encode, tf.transpose(self.W_)) + self.bv_)\n",
    "    \n",
    "    def _create_cost_function_node(self):\n",
    "        with tf.name_scope('cost'):\n",
    "            _triplet_loss = batch_hard_triplet_loss\n",
    "            \n",
    "            self.triplet_loss, data_weight, self.fraction_triplet, self.num_triplet = _triplet_loss(self.sparse_input, self.input_label, self.encode)\n",
    "            self.autoencoder_loss = weighted_loss(self.sparse_input, self.input_data, self.decode, loss_func=self.loss_func, weight=data_weight)\n",
    "            self.cost = self.autoencoder_loss + self.alpha*self.triplet_loss\n",
    "     \n",
    "    def _create_train_step_node(self):\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_step = tf.train.AdamOptimizer(self.learning_rate).minimize(self.cost)\n",
    "\n",
    "    def transform(self, data, name='train'):\n",
    "        with tf.Session() as self.tf_session:\n",
    "            self.tf_saver.restore(self.tf_session, self.model_name)\n",
    "            \n",
    "            if isinstance(data, np.ndarray):\n",
    "                encoded_data = self.encode.eval({self.input_data_corr: data})\n",
    "            else:\n",
    "                encoded_data = self.encode.eval({self.input_data_corr: get_sparse_ind_val_shape(data)})\n",
    "                \n",
    "#             weights = self.W_.eval()\n",
    "            \n",
    "            return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define functions to create the triplet loss with online triplet mining.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def _get_anchor_positive_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.\n",
    "\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i and j are distinct\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "\n",
    "    # Check if labels[i] == labels[j]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "\n",
    "    # Combine the two masks\n",
    "    mask = tf.logical_and(indices_not_equal, labels_equal)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _get_anchor_negative_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
    "\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check if labels[i] != labels[k]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "\n",
    "    mask = tf.logical_not(labels_equal)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _get_triplet_mask(labels):\n",
    "    \"\"\"Return a 3D mask where mask[a, p, n] is True iff the triplet (a, p, n) is valid.\n",
    "\n",
    "    A triplet (i, j, k) is valid if:\n",
    "        - i, j, k are distinct\n",
    "        - labels[i] == labels[j] and labels[i] != labels[k]\n",
    "\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i, j and k are distinct\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "    i_not_equal_j = tf.expand_dims(indices_not_equal, 2)\n",
    "    i_not_equal_k = tf.expand_dims(indices_not_equal, 1)\n",
    "    j_not_equal_k = tf.expand_dims(indices_not_equal, 0)\n",
    "\n",
    "    distinct_indices = tf.logical_and(tf.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)\n",
    "\n",
    "    # Check if labels[i] == labels[j] and labels[i] != labels[k]\n",
    "    label_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    i_equal_j = tf.expand_dims(label_equal, 2)\n",
    "    i_equal_k = tf.expand_dims(label_equal, 1)\n",
    "\n",
    "    valid_labels = tf.logical_and(i_equal_j, tf.logical_not(i_equal_k))\n",
    "\n",
    "    # Combine the two masks\n",
    "    mask = tf.logical_and(distinct_indices, valid_labels)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def batch_all_triplet_loss(sparse_input, input_label, encode, pos_triplets_only = False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "\n",
    "    We generate all the valid triplets and average the loss over the positive ones.\n",
    "\n",
    "    Args:\n",
    "        input_label: labels of the batch, of size (batch_size,)\n",
    "        encode: tensor of shape (batch_size, embed_dim)\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the dot product\n",
    "    dotproduct = tf.matmul(encode, tf.transpose(encode))\n",
    "\n",
    "    # shape (batch_size, batch_size, 1)\n",
    "    anchor_positive_dotproduct = tf.expand_dims(dotproduct, 2)\n",
    "    assert anchor_positive_dotproduct.shape[2] == 1\n",
    "    # shape (batch_size, 1, batch_size)\n",
    "    anchor_negative_dotproduct = tf.expand_dims(dotproduct, 1)\n",
    "    assert anchor_negative_dotproduct.shape[1] == 1\n",
    "\n",
    "    # Compute a 3D tensor of size (batch_size, batch_size, batch_size)\n",
    "    # triplet_loss[i, j, k] will contain the triplet loss of anchor=i, positive=j, negative=k\n",
    "    # Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)\n",
    "    # and the 2nd (batch_size, 1, batch_size)\n",
    "    triplet_distance = - anchor_positive_dotproduct + anchor_negative_dotproduct\n",
    "\n",
    "    # Put to zero the invalid triplets\n",
    "    # (where label(a) != label(p) or label(n) == label(a) or a == p)\n",
    "    valid_triplet_mask = tf.to_float(_get_triplet_mask(input_label))\n",
    "    num_valid_triplets = tf.reduce_sum(valid_triplet_mask)\n",
    "\n",
    "    # Count number of positive triplets (where triplet_distance > 0)\n",
    "    pos_valid_triplet_mask = tf.to_float(tf.greater(tf.multiply(valid_triplet_mask, triplet_distance), 1e-16))\n",
    "    num_pos_valid_triplets = tf.reduce_sum(pos_valid_triplet_mask)\n",
    "\n",
    "    # Set final mask\n",
    "    if pos_triplets_only:\n",
    "        mask = pos_valid_triplet_mask\n",
    "        num_triplet = num_pos_valid_triplets\n",
    "    else:\n",
    "        mask = valid_triplet_mask\n",
    "        num_triplet = num_valid_triplets\n",
    "\n",
    "    # Get final mean triplet loss over the (positive) valid triplets\n",
    "    triplet_loss = - tf.log_sigmoid(-triplet_distance) * mask\n",
    "    triplet_loss = tf.reduce_sum(triplet_loss) / (num_triplet + 1e-16)\n",
    "\n",
    "    data_weight = tf.reduce_sum(mask, [1, 2]) + tf.reduce_sum(mask, [0, 1]) + tf.reduce_sum(mask, [0, 2])\n",
    "\n",
    "    return triplet_loss, data_weight, num_pos_valid_triplets / (num_valid_triplets + 1e-16), num_pos_valid_triplets\n",
    "\n",
    "\n",
    "def batch_all_triplet_loss_org(sparse_input, input_label, encode, input_data, decode, pos_triplets_only = False, autoencoder_loss_func='cross_entropy'):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "\n",
    "    We generate all the valid triplets and average the loss over the positive ones.\n",
    "\n",
    "    Args:\n",
    "        input_label: labels of the batch, of size (batch_size,)\n",
    "        encode: tensor of shape (batch_size, embed_dim)\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the dot product\n",
    "    dotproduct = tf.matmul(encode, tf.transpose(encode))\n",
    "\n",
    "    # shape (batch_size, batch_size, 1)\n",
    "    anchor_positive_dotproduct = tf.expand_dims(dotproduct, 2)\n",
    "    assert anchor_positive_dotproduct.shape[2] == 1\n",
    "    # shape (batch_size, 1, batch_size)\n",
    "    anchor_negative_dotproduct = tf.expand_dims(dotproduct, 1)\n",
    "    assert anchor_negative_dotproduct.shape[1] == 1\n",
    "\n",
    "    # Compute a 3D tensor of size (batch_size, batch_size, batch_size)\n",
    "    # triplet_loss[i, j, k] will contain the triplet loss of anchor=i, positive=j, negative=k\n",
    "    # Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)\n",
    "    # and the 2nd (batch_size, 1, batch_size)\n",
    "    triplet_distance = - anchor_positive_dotproduct + anchor_negative_dotproduct\n",
    "\n",
    "    # Put to zero the invalid triplets\n",
    "    # (where label(a) != label(p) or label(n) == label(a) or a == p)\n",
    "    valid_triplet_mask = tf.to_float(_get_triplet_mask(input_label))\n",
    "    num_valid_triplets = tf.reduce_sum(valid_triplet_mask)\n",
    "\n",
    "    # Count number of positive triplets (where triplet_distance > 0)\n",
    "    pos_valid_triplet_mask = tf.to_float(tf.greater(tf.multiply(valid_triplet_mask, triplet_distance), 1e-16))\n",
    "    num_pos_valid_triplets = tf.reduce_sum(pos_valid_triplet_mask)\n",
    "\n",
    "    # Set final mask\n",
    "    if pos_triplets_only:\n",
    "        mask = pos_valid_triplet_mask\n",
    "        num_triplet = num_pos_valid_triplets\n",
    "    else:\n",
    "        mask = valid_triplet_mask\n",
    "        num_triplet = num_valid_triplets\n",
    "\n",
    "    # Get final mean triplet loss over the (positive) valid triplets\n",
    "    triplet_loss = - tf.log_sigmoid(-triplet_distance) * mask\n",
    "    triplet_loss = tf.reduce_sum(triplet_loss) / (num_triplet + 1e-16)\n",
    "\n",
    "    data_weight = tf.reduce_sum(mask, [1, 2]) + tf.reduce_sum(mask, [0, 1]) + tf.reduce_sum(mask, [0, 2])\n",
    "\n",
    "    # Autoencoder element wise cross entropy loss / mean squared loss\n",
    "    _reduce_sum = tf.sparse.reduce_sum if sparse_input else tf.reduce_sum\n",
    "    _to_dense = tf.sparse.to_dense if sparse_input else lambda x: x\n",
    "\n",
    "    if autoencoder_loss_func == 'cross_entropy':\n",
    "        autoencoder_loss = - tf.reduce_sum(_to_dense(input_data) * tf.log(decode+1e-16) + (1.-_to_dense(input_data)) * tf.log(1.-decode+1e-16), 1)\n",
    "    elif autoencoder_loss_func == 'mean_squared':\n",
    "        autoencoder_loss = tf.reduce_sum(tf.squared_difference(_to_dense(input_data),decode), 1)\n",
    "    elif autoencoder_loss_func == 'cosine_proximity':\n",
    "        autoencoder_loss = - tf.reduce_sum(tf.nn.l2_normalize(_to_dense(input_data),1) * tf.nn.l2_normalize(decode,1), 1)\n",
    "    autoencoder_loss = tf.reduce_sum(autoencoder_loss * data_weight) / (tf.reduce_sum(data_weight) + 1e-16)\n",
    "    # autoencoder_loss = tf.reduce_mean(autoencoder_loss)  # using this will make it becomes normal autoencoder loss\n",
    "\n",
    "    return triplet_loss, autoencoder_loss, num_pos_valid_triplets / (num_valid_triplets + 1e-16), num_pos_valid_triplets\n",
    "\n",
    "\n",
    "def batch_hard_triplet_loss(sparse_input, input_label, encode):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "\n",
    "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
    "\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the pairwise distance matrix\n",
    "    dotproduct = tf.matmul(encode, tf.transpose(encode))\n",
    "\n",
    "    # For each anchor, get the hardest positive (similar items with smallest dotproduct)\n",
    "    # First, we need to get a mask for every valid positive (they should have same label)\n",
    "    mask_anchor_positive = _get_anchor_positive_triplet_mask(input_label)\n",
    "    mask_anchor_positive = tf.to_float(mask_anchor_positive)\n",
    "\n",
    "    # We add the maximum value in each row to the invalid positives (label(a) != label(n))\n",
    "    max_anchor_dotproduct = tf.reduce_max(dotproduct, axis=1, keepdims=True)\n",
    "    anchor_positive_dotproduct = dotproduct + max_anchor_dotproduct * (1.0 - mask_anchor_positive)\n",
    "\n",
    "    # shape (batch_size, 1)\n",
    "    hardest_positive_dotproduct = tf.reduce_min(anchor_positive_dotproduct, axis=1, keepdims=True)\n",
    "    tf.summary.scalar(\"hardest_positive_dotproduct\", tf.reduce_mean(hardest_positive_dotproduct))\n",
    "\n",
    "    # For each anchor, get the hardest negative (dissimilar items with largest dotproduct)\n",
    "    # First, we need to get a mask for every valid negative (they should have different labels)\n",
    "    mask_anchor_negative = _get_anchor_negative_triplet_mask(input_label)\n",
    "    mask_anchor_negative = tf.to_float(mask_anchor_negative)\n",
    "\n",
    "    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n",
    "    anchor_negative_dotproduct = tf.multiply(mask_anchor_negative, dotproduct)\n",
    "\n",
    "    # shape (batch_size,1)\n",
    "    hardest_negative_dotproduct = tf.reduce_max(anchor_negative_dotproduct, axis=1, keepdims=True)\n",
    "    tf.summary.scalar(\"hardest_negative_dotproduct\", tf.reduce_mean(hardest_negative_dotproduct))\n",
    "\n",
    "    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
    "    triplet_dist = tf.maximum(hardest_negative_dotproduct - hardest_positive_dotproduct, 0.0)\n",
    "\n",
    "    triplet_count = tf.to_float(tf.greater(triplet_dist,0.0))\n",
    "\n",
    "    data_weight = tf.squeeze(triplet_count) + \\\n",
    "                  tf.reduce_sum(triplet_count * tf.to_float(tf.equal(dotproduct, hardest_positive_dotproduct)),0) + \\\n",
    "                  tf.reduce_sum(triplet_count * tf.to_float(tf.equal(dotproduct, hardest_negative_dotproduct)), 0)\n",
    "\n",
    "    # Get final mean triplet loss\n",
    "    triplet_loss = - tf.log_sigmoid(-triplet_dist) * triplet_count\n",
    "    triplet_loss = tf.reduce_sum(triplet_loss) / (tf.reduce_sum(triplet_count) + 1e-16)\n",
    "\n",
    "    return triplet_loss, data_weight, tf.reduce_sum(triplet_count) / tf.to_float(tf.shape(input_label)[0]), tf.reduce_sum(triplet_count)\n",
    "\n",
    "\n",
    "def weighted_loss(sparse_input, input_data, decode, loss_func='cross_entropy', weight=None):\n",
    "    _reduce_sum = tf.sparse.reduce_sum if sparse_input else tf.reduce_sum\n",
    "    _to_dense = tf.sparse.to_dense if sparse_input else lambda x: x\n",
    "\n",
    "    if weight is None: weight = tf.ones(tf.shape(input_data)[0])\n",
    "\n",
    "    if loss_func == 'cross_entropy':\n",
    "        autoencoder_loss = - tf.reduce_sum(_to_dense(input_data) * tf.log(decode+1e-16) + (1.-_to_dense(input_data)) * tf.log(1.-decode+1e-16), 1)\n",
    "    elif loss_func == 'mean_squared':\n",
    "        autoencoder_loss = tf.reduce_sum(tf.squared_difference(_to_dense(input_data),decode), 1)\n",
    "    elif loss_func == 'cosine_proximity':\n",
    "        autoencoder_loss = - tf.reduce_sum(tf.nn.l2_normalize(_to_dense(input_data),1) * tf.nn.l2_normalize(decode,1), 1)\n",
    "\n",
    "    autoencoder_loss = tf.reduce_sum(autoencoder_loss * weight) / (tf.reduce_sum(weight) + 1e-16)\n",
    "\n",
    "    return autoencoder_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist = input_data.read_data_sets('MNIST_data/')\n",
    "# x_train, y_train = mnist.train.images, mnist.train.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./newsCorpora.csv', header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={1: 'title', 4: 'category'})\n",
    "df = df[['title', 'category']]\n",
    "# TODO: Remove stopwords from titles.\n",
    "def clean(x):\n",
    "    for token in x.split():\n",
    "        pass\n",
    "df.title = df.title.map(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.category.values\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           b       0.85      0.87      0.86     38269\n",
      "           e       0.90      0.93      0.92     50315\n",
      "           m       0.86      0.81      0.84     15061\n",
      "           t       0.88      0.85      0.87     35754\n",
      "\n",
      "   micro avg       0.88      0.88      0.88    139399\n",
      "   macro avg       0.87      0.87      0.87    139399\n",
      "weighted avg       0.88      0.88      0.88    139399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[54637,5463] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node random_uniform_2/RandomUniform (defined at <ipython-input-4-5abf60a8e94c>:4)  = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'random_uniform_2/RandomUniform', defined at:\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\base_events.py\", line 427, in run_forever\n    self._run_once()\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\base_events.py\", line 1440, in _run_once\n    handle._run()\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3191, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-8c168856a8ee>\", line 2, in <module>\n    dae.fit(x_train, train_set_label=y_train)\n  File \"<ipython-input-5-cb710f2e1073>\", line 22, in fit\n    self._build_model(n_features)\n  File \"<ipython-input-5-cb710f2e1073>\", line 85, in _build_model\n    self.W_, self.bh_, self.bv_ = self._create_variables(n_features)\n  File \"<ipython-input-5-cb710f2e1073>\", line 101, in _create_variables\n    W_ = tf.Variable(xavier_init(n_features, self.n_components, self.xavier_init), name='enc-W')\n  File \"<ipython-input-4-5abf60a8e94c>\", line 4, in xavier_init\n    return tf.random_uniform((n_features, n_components), minval=low, maxval=high)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\random_ops.py\", line 243, in random_uniform\n    rnd = gen_random_ops.random_uniform(shape, dtype, seed=seed1, seed2=seed2)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_random_ops.py\", line 771, in random_uniform\n    name=name)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[54637,5463] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node random_uniform_2/RandomUniform (defined at <ipython-input-4-5abf60a8e94c>:4)  = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[54637,5463] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node random_uniform_2/RandomUniform}} = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-8c168856a8ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDenoisingAutoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-2ae41a549913>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, train_set, validation_set, train_set_label, validation_set_label)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_session\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_tf_utilities_and_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_set_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_set_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-2ae41a549913>\u001b[0m in \u001b[0;36m_initialize_tf_utilities_and_ops\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0minit_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_saver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_set_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[54637,5463] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node random_uniform_2/RandomUniform (defined at <ipython-input-4-5abf60a8e94c>:4)  = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'random_uniform_2/RandomUniform', defined at:\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\base_events.py\", line 427, in run_forever\n    self._run_once()\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\base_events.py\", line 1440, in _run_once\n    handle._run()\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3191, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-8c168856a8ee>\", line 2, in <module>\n    dae.fit(x_train, train_set_label=y_train)\n  File \"<ipython-input-5-cb710f2e1073>\", line 22, in fit\n    self._build_model(n_features)\n  File \"<ipython-input-5-cb710f2e1073>\", line 85, in _build_model\n    self.W_, self.bh_, self.bv_ = self._create_variables(n_features)\n  File \"<ipython-input-5-cb710f2e1073>\", line 101, in _create_variables\n    W_ = tf.Variable(xavier_init(n_features, self.n_components, self.xavier_init), name='enc-W')\n  File \"<ipython-input-4-5abf60a8e94c>\", line 4, in xavier_init\n    return tf.random_uniform((n_features, n_components), minval=low, maxval=high)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\random_ops.py\", line 243, in random_uniform\n    rnd = gen_random_ops.random_uniform(shape, dtype, seed=seed1, seed2=seed2)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_random_ops.py\", line 771, in random_uniform\n    name=name)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"c:\\users\\young.a.lee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[54637,5463] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node random_uniform_2/RandomUniform (defined at <ipython-input-4-5abf60a8e94c>:4)  = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "dae = DenoisingAutoencoder()\n",
    "dae.fit(x_train, train_set_label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./mnist/\n",
      "INFO:tensorflow:Restoring parameters from ./mnist/\n"
     ]
    }
   ],
   "source": [
    "embed_train = dae.transform(x_train)\n",
    "embed_test = dae.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib import offsetbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_embedding(X, y, imgs=None, title=None):\n",
    "#     # Adapted from http://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html\n",
    "#     x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "#     X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "#     # Plot colors numbers\n",
    "#     plt.figure(figsize=(10,10))\n",
    "#     ax = plt.subplot(111)\n",
    "#     for i in range(X.shape[0]):\n",
    "#         # plot colored number\n",
    "#         plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
    "#                  color=plt.cm.Set1(y[i] / 10.),\n",
    "#                  fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "#     # Add image overlays\n",
    "#     if imgs is not None and hasattr(offsetbox, 'AnnotationBbox'):\n",
    "#         # only print thumbnails with matplotlib > 1.0\n",
    "#         shown_images = np.array([[1., 1.]])  # just something big\n",
    "#         for i in range(X.shape[0]):\n",
    "#             dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "#             if np.min(dist) < 4e-3:\n",
    "#                 # don't show points that are too close\n",
    "#                 continue\n",
    "#             shown_images = np.r_[shown_images, [X[i]]]\n",
    "#             imagebox = offsetbox.AnnotationBbox(\n",
    "#                 offsetbox.OffsetImage(imgs[i], cmap=plt.cm.gray_r), X[i])\n",
    "#             ax.add_artist(imagebox)\n",
    "\n",
    "#     plt.xticks([]), plt.yticks([])\n",
    "#     if title is not None:\n",
    "#         plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(gamma='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(embed_train, y_train)\n",
    "y_pred = clf.predict(embed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       980\n",
      "           1       0.98      0.99      0.99      1135\n",
      "           2       0.95      0.95      0.95      1032\n",
      "           3       0.95      0.96      0.95      1010\n",
      "           4       0.94      0.96      0.95       982\n",
      "           5       0.95      0.95      0.95       892\n",
      "           6       0.97      0.97      0.97       958\n",
      "           7       0.96      0.95      0.96      1028\n",
      "           8       0.95      0.94      0.95       974\n",
      "           9       0.95      0.92      0.93      1009\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     10000\n",
      "   macro avg       0.96      0.96      0.96     10000\n",
      "weighted avg       0.96      0.96      0.96     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classification_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d57559b38c5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'classification_report' is not defined"
     ]
    }
   ],
   "source": [
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
